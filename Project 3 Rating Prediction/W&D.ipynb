{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import sqrt\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" #comment out this line if you want to use gpu\n",
    "import random\n",
    "from keras.layers import Concatenate, Dense, Dot, Dropout, Embedding, Input, Reshape\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2021)\n",
    "np.random.seed(2021)\n",
    "# tf > 2.0\n",
    "tensorflow.random.set_seed(2021)\n",
    "#tf < 2.0\n",
    "#tf.set_random_seed(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -pred: an array containing all predicted ratings\n",
    "    -actual: an array containing all ground truth ratings\n",
    "    \n",
    "return:\n",
    "    a scalar whose value is the rmse\n",
    "'''\n",
    "def rmse(pred, actual):\n",
    "    # Ignore ratings with value zero.\n",
    "    pred = pred[actual.nonzero()].flatten()\n",
    "    actual = actual[actual.nonzero()].flatten()\n",
    "    return sqrt(mean_squared_error(pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -len_continuous: number of continuous features\n",
    "    -deep_vocab_lens: an array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep \n",
    "        categorical feature\n",
    "    -len_wide: number of wide features\n",
    "    -embed_size: dimension of the embedding vectors of deep categorical features\n",
    "    \n",
    "return:\n",
    "    a keras Model object for the constructed wdl model \n",
    "'''\n",
    "\n",
    "\n",
    "def build_wdl_model(len_continuous, deep_vocab_lens, len_wide, embed_size):\n",
    "    # A list containing all input layers\n",
    "    input_list = []\n",
    "    \n",
    "    # Input layer for continuous features\n",
    "    continuous_input = Input(shape=(len_continuous,), dtype='float32', name='continuous_input')\n",
    "    input_list.append(continuous_input)\n",
    "    \n",
    "    \n",
    "    # Get embeddings for all deep categorical features\n",
    "    emb_list = []\n",
    "    for vocab_size in deep_vocab_lens:\n",
    "        _input = Input(shape=(1,), dtype='int32')\n",
    "        input_list.append(_input)\n",
    "        _emb = Embedding(output_dim=embed_size, input_dim=vocab_size, input_length=1)(_input)\n",
    "        _emb = Reshape((embed_size,))(_emb)\n",
    "        emb_list.append(_emb)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # Create input layer for deep component by concatenating the embeddings and continuous features' input layer\n",
    "    deep_input = Concatenate()(emb_list + [continuous_input])\n",
    "    \n",
    "\n",
    "    # Construct deep component\n",
    "    dense_1 = Dense(256, activation='relu')(deep_input)\n",
    "    dense_1_dp = Dropout(0.3)(dense_1)\n",
    "    dense_2 = Dense(128, activation='relu')(dense_1_dp)\n",
    "    dense_2_dp = Dropout(0.3)(dense_2)\n",
    "    dense_3 = Dense(64, activation='relu')(dense_2_dp)\n",
    "    dense_3_dp = Dropout(0.3)(dense_3)\n",
    "\n",
    "    \n",
    "    # Create input layer for wide component\n",
    "    wide_input = Input(shape=(len_wide,), dtype='float32')\n",
    "    input_list.append(wide_input)\n",
    "\n",
    "    \n",
    "    # Concatenate the outputs of deep and wide components and feed the \n",
    "    # concatenated vector into the finall fully connected layer\n",
    "    fc_input = Concatenate()([dense_3_dp, wide_input])\n",
    "    model_output = Dense(1)(fc_input)\n",
    "    \n",
    "    model = Model(inputs=input_list,\n",
    "                  outputs=model_output)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -continuous_columns: column names of continuous features\n",
    "    \n",
    "return: \n",
    "    a numpy array where each row contains the values of continuous features in the corresponding row of the\n",
    "    input dataframe\n",
    "'''\n",
    "def get_continuous_features(df, continuous_columns):\n",
    "    continuous_features = df[continuous_columns].values\n",
    "    return continuous_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -comb_p: number of elements in each combination (e.g., there are two elements in the combination {fried chicken, chicken and \n",
    "    waffle}, and three elements in the combination {fried chicken, chicken and waffle, chicken fried rice})\n",
    "    -topk: number of mostly frequent combinations to retrieve\n",
    "    -output_freq: whether to return the frequencies of retrieved combinations\n",
    "    \n",
    "return:\n",
    "    1. output_freq = True: a list X where each element is a tuple containing a combinantion tuple and corresponding frequency, and the \n",
    "        elements are stored in the descending order of their frequencies\n",
    "    2. output_freq = False: a list X where each element is a tuple containing a combinantion tuple, and the elements are stored in \n",
    "    the descending order of their frequencies\n",
    "'''\n",
    "def get_top_k_p_combinations(df, comb_p, topk, output_freq=False):\n",
    "    # get all combinations with comb_p\n",
    "    def get_category_combinations(categories_str, comb_p=2):\n",
    "        categories = categories_str.split(', ')\n",
    "        return list(combinations(categories, comb_p))\n",
    "    # [('Lounges', 'Dance Clubs'), ('Lounges', 'Bars'), ('Lounges', 'Nightlife'), ('Dance Clubs', 'Bars'), ('Dance Clubs', 'Nightlife'), ('Bars', 'Nightlife')]\n",
    "    all_categories_p_combos = df[\"item_categories\"].apply(\n",
    "        lambda x: get_category_combinations(x, comb_p)).values.tolist()\n",
    "    # ('Lounges', 'Dance Clubs')\n",
    "    # list of tuples that each index refer to one combination\n",
    "    all_categories_p_combos = [tuple(t) for item in all_categories_p_combos for t in item]\n",
    "\n",
    "    tmp = dict(Counter(all_categories_p_combos))\n",
    "    sorted_categories_combinations = list(sorted(tmp.items(), key=lambda x: x[1], reverse=True))\n",
    "    if output_freq:\n",
    "        return sorted_categories_combinations[:topk]\n",
    "    else:\n",
    "        return [t[0] for t in sorted_categories_combinations[:topk]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "    -df: input dataframe\n",
    "    -selected_categories_to_idx: a dictionary mapping item categories to corrresponding integral indices\n",
    "    -top_combinations: a list containing retrieved mostly frequent combinantions of item categories\n",
    "    \n",
    "return:\n",
    "    a numpy array where each row contains the categorical features' binary encodings and cross product\n",
    "    transformations for the corresponding row of the input dataframe\n",
    "'''\n",
    "\n",
    "def get_wide_features(df, selected_categories_to_idx, top_combinations):\n",
    "    def categories_to_binary_output(categories):\n",
    "        binary_output = [0 for _ in range(len(selected_categories_to_idx))]\n",
    "        for category in categories.split(', '):\n",
    "            if category in selected_categories_to_idx:\n",
    "                binary_output[selected_categories_to_idx[category]] = 1\n",
    "            else:\n",
    "                binary_output[0] = 1\n",
    "        return binary_output\n",
    "    def categories_cross_transformation(categories):\n",
    "        current_category_set = set(categories.split(', '))\n",
    "        corss_transform_output = [0 for _ in range(len(top_combinations))]\n",
    "        for k, comb_k in enumerate(top_combinations):\n",
    "            if len(current_category_set & comb_k) == len(comb_k):\n",
    "                corss_transform_output[k] = 1\n",
    "            else:\n",
    "                corss_transform_output[k] = 0\n",
    "        return corss_transform_output\n",
    "\n",
    "    category_binary_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_to_binary_output(x)).values.tolist())\n",
    "    print('category_binary_features shape:',category_binary_features.shape)\n",
    "    category_corss_transform_features = np.array(df.item_categories.apply(\n",
    "        lambda x: categories_cross_transformation(x)).values.tolist())\n",
    "    print('category_cross_features shape:',category_corss_transform_features.shape)\n",
    "    out = np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n",
    "    print('wide features shape:',out.shape)\n",
    "    return np.concatenate((category_binary_features, category_corss_transform_features), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = pd.read_csv(\"data/train.csv\")\n",
    "val_df = pd.read_csv(\"data/valid.csv\")\n",
    "te_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "tr_ratings = tr_df.stars.values\n",
    "val_ratings = val_df.stars.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv(\"data/user.csv\")\n",
    "item_df = pd.read_csv(\"data/business.csv\")\n",
    "\n",
    "# Rename some columns of dfs and convert the indices of dfs into string type for easier reference in later stage \n",
    "user_df = user_df.rename(index=str, columns={t: 'user_' + t for t in user_df.columns if t != 'user_id'})\n",
    "item_df = item_df.rename(index=str, columns={t: 'item_' + t for t in item_df.columns if t != 'business_id'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the original row indices of each rating table\n",
    "tr_df[\"index\"] = tr_df.index\n",
    "val_df[\"index\"]  = val_df.index\n",
    "te_df[\"index\"] = te_df.index\n",
    "\n",
    "tr_df = pd.merge(pd.merge(tr_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "val_df = pd.merge(pd.merge(val_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "te_df = pd.merge(pd.merge(te_df, user_df, on='user_id'), item_df, on='business_id').sort_values(by=['index']).reset_index(drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the columns containing conitnuous features\n",
    "continuous_columns = [\"user_average_stars\", \"user_cool\", \"user_fans\", \n",
    "                      \"user_review_count\", \"user_useful\", \"user_funny\",\n",
    "                      \"item_is_open\", \"item_latitude\", \"item_longitude\", \n",
    "                      \"item_review_count\", \"item_stars\"]\n",
    "\n",
    "# Get values of continous features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_continuous_features = get_continuous_features(tr_df, continuous_columns)\n",
    "val_continuous_features = get_continuous_features(val_df, continuous_columns)\n",
    "te_continuous_features = get_continuous_features(te_df, continuous_columns)\n",
    "\n",
    "# Standardize each feature by removing the mean of the training samples and scaling to unit variance.\n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html for more details.\n",
    "scaler = StandardScaler().fit(tr_continuous_features)\n",
    "\n",
    "tr_continuous_features = scaler.transform(tr_continuous_features)\n",
    "val_continuous_features = scaler.transform(val_continuous_features)\n",
    "te_continuous_features = scaler.transform(te_continuous_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sepcify column names of deep categorical features\n",
    "item_deep_columns = [\"item_city\", \"item_postal_code\", \"item_state\"]\n",
    "\n",
    "# An array of integers where deep_vocab_lens[i] represents the number of unique values of (i+1)-th deep categorical feature\n",
    "item_deep_vocab_lens = []\n",
    "\n",
    "for col_name in item_deep_columns:\n",
    "    # Get all unique values of this deep categorical feature\n",
    "    tmp = item_df[col_name].unique()\n",
    "    \n",
    "    # Create a dictionary mapping each unique value to a unique integral index\n",
    "    vocab = dict(zip(tmp, range(1, len(tmp) + 1)))\n",
    "    \n",
    "    # Get the number of unique values of this deep categorical features\n",
    "    item_deep_vocab_lens.append(len(vocab) + 1)\n",
    "    \n",
    "    # Create a new column where each entry stores the integral index of this deep categorical feature's value in the same row\n",
    "    item_df[col_name + \"_idx\"] = item_df[col_name].apply(lambda x: vocab[x])\n",
    "\n",
    "\n",
    "# Create a dictionary mapping each business id to corresponding values of deep categorical features\n",
    "item_deep_idx_columns = [t + \"_idx\" for t in item_deep_columns]\n",
    "item_to_deep_categorical_features = dict(zip(item_df.business_id.values, item_df[item_deep_idx_columns].values.tolist()))\n",
    "\n",
    "# Creat numpy arrays storing corresponding deep categorical features' values of train/validation/test sets using the above mapping\n",
    "tr_deep_categorical_features = np.array(tr_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "val_deep_categorical_features = np.array(val_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n",
    "te_deep_categorical_features = np.array(te_df.business_id.apply(lambda x: item_to_deep_categorical_features[x]).values.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categories of all items \n",
    "all_categories = [category for category_list in item_df.item_categories.values for category in category_list.split(\", \")]\n",
    "\n",
    "# Sort all unique values of the item categories by their frequencies in descending order\n",
    "category_sorted = sorted(Counter(all_categories).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select top 500 most frequent categories\n",
    "selected_categories = [t[0] for t in category_sorted[:500]]\n",
    "\n",
    "# Create a dictionary mapping each secleted category to a unique integral index\n",
    "selected_categories_to_idx = dict(zip(selected_categories, range(1, len(selected_categories) + 1)))\n",
    "\n",
    "# Map all categories unseen in the item df to index 0\n",
    "selected_categories_to_idx['unk'] = 0\n",
    "\n",
    "# Create a dictionary mapping each integral index to corresponding category\n",
    "idx_to_selected_categories = {val: key for key, val in selected_categories_to_idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get most frequent categories combinantions using the utility function defined previously and store them in the folloing list\n",
    "top_combinations = []\n",
    "\n",
    "# Get top 50 most frequent two-categories combinantions in the train set\n",
    "\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 2, 50, output_freq=False)\n",
    "\n",
    "# Get top 30 most frequent three-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 3, 30, output_freq=False)\n",
    "\n",
    "# Get top 20 most frequent four-categories combinantions in the train set\n",
    "top_combinations += get_top_k_p_combinations(tr_df, 4, 20, output_freq=False)\n",
    "\n",
    "# Convert each combinantion in the list to a set data structure\n",
    "top_combinations = [set(t) for t in top_combinations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_binary_features shape: (60080, 501)\n",
      "category_cross_features shape: (60080, 100)\n",
      "wide features shape: (60080, 601)\n",
      "category_binary_features shape: (7510, 501)\n",
      "category_cross_features shape: (7510, 100)\n",
      "wide features shape: (7510, 601)\n",
      "category_binary_features shape: (7510, 501)\n",
      "category_cross_features shape: (7510, 100)\n",
      "wide features shape: (7510, 601)\n"
     ]
    }
   ],
   "source": [
    "# Get values of wide features for train/validation/test sets using the utility function defined previously\n",
    "\n",
    "tr_wide_features = get_wide_features(tr_df, selected_categories_to_idx, top_combinations)\n",
    "val_wide_features = get_wide_features(val_df, selected_categories_to_idx, top_combinations)\n",
    "te_wide_features = get_wide_features(te_df, selected_categories_to_idx, top_combinations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_wide_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_features = [tr_continuous_features,tr_deep_categorical_features[:,0],tr_deep_categorical_features[:,1],tr_deep_categorical_features[:,2],tr_wide_features]\n",
    "# tr_features = []\n",
    "# tr_features.append(tr_continuous_features.tolist())\n",
    "# tr_features += [tr_deep_categorical_features[:,i].tolist() for i in range(tr_deep_categorical_features.shape[1])]\n",
    "# tr_features.append(tr_wide_features.tolist())\n",
    "\n",
    "val_features = [val_continuous_features,val_deep_categorical_features[:,0],val_deep_categorical_features[:,1],val_deep_categorical_features[:,2],val_wide_features]\n",
    "# val_features = []\n",
    "# val_features.append(val_continuous_features.tolist())\n",
    "# val_features += [val_deep_categorical_features[:,i].tolist() for i in range(val_deep_categorical_features.shape[1])]\n",
    "# val_features.append(val_wide_features.tolist())\n",
    "\n",
    "te_features = []\n",
    "te_features.append(te_continuous_features.tolist())\n",
    "te_features += [te_deep_categorical_features[:,i].tolist() for i in range(te_deep_categorical_features.shape[1])]\n",
    "te_features.append(te_wide_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-1.3106592 , -0.29126561, -0.29606097, ..., -0.62640436,\n",
       "         -0.32860267,  0.27585547],\n",
       "        [ 1.8933461 ,  0.27159911,  0.3575088 , ..., -0.38901381,\n",
       "         -0.44103617,  0.27585547],\n",
       "        [ 0.5683815 , -0.26975485, -0.16718806, ..., -0.62240987,\n",
       "         -0.31827715, -0.69945181],\n",
       "        ...,\n",
       "        [-0.22659725, -0.28051023, -0.15798285, ...,  1.91534585,\n",
       "         -0.59706634, -0.69945181],\n",
       "        [-1.11793708, -0.08998641,  0.08135256, ...,  1.92068597,\n",
       "         -0.59821362, -1.6747591 ],\n",
       "        [-0.73249283, -0.3097034 , -0.30526618, ...,  1.87761793,\n",
       "         -0.20469636,  1.25116276]]),\n",
       " array([ 8, 21,  8, ...,  6, 10,  1]),\n",
       " array([ 86, 193,   8, ..., 470, 655, 189]),\n",
       " array([3, 2, 3, ..., 5, 5, 1]),\n",
       " array([[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdl_model = build_wdl_model(\n",
    "        len(tr_continuous_features[0]),\n",
    "        item_deep_vocab_lens,   # num of category classes\n",
    "        len(tr_wide_features[0]), \n",
    "        embed_size=100)\n",
    "#print(len(tr_continuous_features[0]))\n",
    "#print(item_deep_vocab_lens)\n",
    "#print(len(tr_wide_features[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.1569\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1968\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0966\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0738\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0669\n"
     ]
    }
   ],
   "source": [
    "wdl_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "history = wdl_model.fit(\n",
    "        tr_features, \n",
    "        tr_ratings, \n",
    "        epochs=5, verbose=1, callbacks=[ModelCheckpoint('model.h5')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN RMSE:  0.9945750224254961\n",
      "VALID RMSE:  1.0047115652332168\n"
     ]
    }
   ],
   "source": [
    "y_pred = wdl_model.predict(tr_features)\n",
    "print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "y_pred = wdl_model.predict(val_features)\n",
    "print(\"VALID RMSE: \", rmse(y_pred, val_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.7990\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.7612\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.6048\n",
      "optimizer adagrad\n",
      "epoch 3\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0822131114990294\n",
      "VALID RMSE:  1.0670337077548389\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.6780\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.6819\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.5507\n",
      "optimizer adagrad\n",
      "epoch 3\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0671137363765368\n",
      "VALID RMSE:  1.051709342225936\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 6.2486\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.6603\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5604\n",
      "optimizer adagrad\n",
      "epoch 3\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0623555294601716\n",
      "VALID RMSE:  1.0477377599473197\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 6.0928\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.6104\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4939\n",
      "optimizer adagrad\n",
      "epoch 3\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.0505639482352687\n",
      "VALID RMSE:  1.0385891351412135\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 4.9966\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5782\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4714\n",
      "optimizer adagrad\n",
      "epoch 3\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.051698500294785\n",
      "VALID RMSE:  1.0329083812904194\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.1895\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.7510\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.5950\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5162\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4931\n",
      "optimizer adagrad\n",
      "epoch 5\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0500992415856558\n",
      "VALID RMSE:  1.038114522146715\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.7541\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.7417\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5845\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5205\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4869\n",
      "optimizer adagrad\n",
      "epoch 5\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.046321987641157\n",
      "VALID RMSE:  1.0346301380661833\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 6.2709\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.6758\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5597\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5153\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4599\n",
      "optimizer adagrad\n",
      "epoch 5\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.039673904811463\n",
      "VALID RMSE:  1.0271376650566246\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.0830\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5991\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4688\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4361\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4037\n",
      "optimizer adagrad\n",
      "epoch 5\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.031036879174211\n",
      "VALID RMSE:  1.0222337032822035\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.6338\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5835\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.4646\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4395\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4194\n",
      "optimizer adagrad\n",
      "epoch 5\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.0288308005518558\n",
      "VALID RMSE:  1.0176575565264483\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.3340\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.7550\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5745\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5264\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4789\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.4383\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4211\n",
      "optimizer adagrad\n",
      "epoch 7\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0426139037124884\n",
      "VALID RMSE:  1.0290162567629557\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.1982\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.7384\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5750\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5205\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4824\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4507\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4247\n",
      "optimizer adagrad\n",
      "epoch 7\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0399073961951761\n",
      "VALID RMSE:  1.0257314696790985\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 4.9825\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.6484\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5080\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4758\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4399\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3989\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3805\n",
      "optimizer adagrad\n",
      "epoch 7\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0335538116018814\n",
      "VALID RMSE:  1.0203461405225924\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 5.7723\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.6494\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5601\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5161\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4839\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4436\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4421\n",
      "optimizer adagrad\n",
      "epoch 7\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.0326155584621932\n",
      "VALID RMSE:  1.0236720391098564\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.0611\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5866\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.4783\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4425\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4190\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3923\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.3695\n",
      "optimizer adagrad\n",
      "epoch 7\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.025843541763538\n",
      "VALID RMSE:  1.0130843529353435\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.4577\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.7494\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.5963\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5192\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5030\n",
      "Epoch 6/9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4610\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.4239\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.4140\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3961\n",
      "optimizer adagrad\n",
      "epoch 9\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0394232075645562\n",
      "VALID RMSE:  1.026867327494554\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.2127\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.7450\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5900\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.5279\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4885\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4548\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4359\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3998\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3858\n",
      "optimizer adagrad\n",
      "epoch 9\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0325844566164548\n",
      "VALID RMSE:  1.0237570292790779\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 5.8884\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.6575\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5258\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4819\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4401\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4075\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3953\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3726\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3619\n",
      "optimizer adagrad\n",
      "epoch 9\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0264956386496746\n",
      "VALID RMSE:  1.0151788710769338\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 5.4147\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5853\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4845\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4487\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4262\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3890\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3838\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3635\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3598\n",
      "optimizer adagrad\n",
      "epoch 9\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.0260059468294662\n",
      "VALID RMSE:  1.01752420121905\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.5185\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5880\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.4813\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4408\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4300\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3999\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3794\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3602\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3489\n",
      "optimizer adagrad\n",
      "epoch 9\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.0204694928466511\n",
      "VALID RMSE:  1.012432037404242\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 4.6750\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.7451\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.5951\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5107\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4835\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4374\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4002\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3954\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3756\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3859\n",
      "optimizer adagrad\n",
      "epoch 10\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0353684122955327\n",
      "VALID RMSE:  1.023861750333966\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 6.2819\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.7643\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.6088\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5439\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.5229\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4787\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4351\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4316\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4039\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4043\n",
      "optimizer adagrad\n",
      "epoch 10\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0304926790136972\n",
      "VALID RMSE:  1.01889474134202\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 5.7364\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.6470\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.5314\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4838\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4675\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4215\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4083\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3745\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3762\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3755\n",
      "optimizer adagrad\n",
      "epoch 10\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0239158629031566\n",
      "VALID RMSE:  1.0141348180124263\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 5.7769\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.6284\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.5241\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.4950\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4598\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4314\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4187\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3987\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3852\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4012\n",
      "optimizer adagrad\n",
      "epoch 10\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.024410859356037\n",
      "VALID RMSE:  1.0167386763326285\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 5.6387\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.6229\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.5037\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.4612\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.4543\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.4076\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.3889\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.3709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.3610\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.3646\n",
      "optimizer adagrad\n",
      "epoch 10\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.0228028159797045\n",
      "VALID RMSE:  1.013369643116387\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 6.5950\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.8145\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.6457\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5629\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5341\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5003\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4607\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4405\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4237\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4195\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3938\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3844\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3822\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 6333s 3s/step - loss: 1.3870\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 12526s 7s/step - loss: 1.3679\n",
      "optimizer adagrad\n",
      "epoch 15\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0273068027776906\n",
      "VALID RMSE:  1.0160385845413582\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 12s 5ms/step - loss: 5.3090: 0\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 9s 5ms/step - loss: 1.7578\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 11s 6ms/step - loss: 1.6031\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 7s 3ms/step - loss: 1.5562\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.5234\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 7s 4ms/step - loss: 1.4751\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 7s 3ms/step - loss: 1.4573\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.4345\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.4090\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4130\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3929\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3678\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3700\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3657\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 2s 1ms/step - loss: 1.3439\n",
      "optimizer adagrad\n",
      "epoch 15\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0241122602179933\n",
      "VALID RMSE:  1.017068693621799\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 7.2783\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.7145\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5875\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5397\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4901\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4729\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4336\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.4240\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4012\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4133\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4060\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3820\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3756\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3733\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.3531\n",
      "optimizer adagrad\n",
      "epoch 15\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0212725794955775\n",
      "VALID RMSE:  1.0123110861419524\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.3490\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.6458\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5044\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.4629\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4378\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4038\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3831\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3681\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.3612\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3684\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3475\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3301\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3346\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3276\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3035\n",
      "optimizer adagrad\n",
      "epoch 15\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.0167441864434215\n",
      "VALID RMSE:  1.0073321040542942\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 5.4750\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.5863\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.4793\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4546\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.4233\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3930\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3934\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3654\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3554\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3546\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3389\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3359\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.3272\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.3333\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2991\n",
      "optimizer adagrad\n",
      "epoch 15\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.0168096554998256\n",
      "VALID RMSE:  1.009579622907518\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.3135\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2178\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.1126\n",
      "optimizer adam\n",
      "epoch 3\n",
      "embed_size 25\n",
      "TRAIN RMSE:  1.0026861667071583\n",
      "VALID RMSE:  1.0076476704761341\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.2061\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2267\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1202\n",
      "optimizer adam\n",
      "epoch 3\n",
      "embed_size 50\n",
      "TRAIN RMSE:  1.0041675742185887\n",
      "VALID RMSE:  1.0088960881367979\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.1471\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 7s 4ms/step - loss: 1.2128\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 7s 4ms/step - loss: 1.1088\n",
      "optimizer adam\n",
      "epoch 3\n",
      "embed_size 100\n",
      "TRAIN RMSE:  1.0020953066165013\n",
      "VALID RMSE:  1.0091172899240752\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.0705\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.2115\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1097\n",
      "optimizer adam\n",
      "epoch 3\n",
      "embed_size 150\n",
      "TRAIN RMSE:  1.0037064616820897\n",
      "VALID RMSE:  1.0084302007691774\n",
      "Epoch 1/3\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 2.0754\n",
      "Epoch 2/3\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.2334\n",
      "Epoch 3/3\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.1191\n",
      "optimizer adam\n",
      "epoch 3\n",
      "embed_size 200\n",
      "TRAIN RMSE:  1.0066459483661403\n",
      "VALID RMSE:  1.013600004010235\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 2.2144\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.2112\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.1069\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0785\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0640\n",
      "optimizer adam\n",
      "epoch 5\n",
      "embed_size 25\n",
      "TRAIN RMSE:  0.9942142390228961\n",
      "VALID RMSE:  1.0054330501372644\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 2.2592\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2160\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.1092\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0823\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0680\n",
      "optimizer adam\n",
      "epoch 5\n",
      "embed_size 50\n",
      "TRAIN RMSE:  0.9941594833095474\n",
      "VALID RMSE:  1.00754722827155\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.1602\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.2162\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1076\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0810\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0652\n",
      "optimizer adam\n",
      "epoch 5\n",
      "embed_size 100\n",
      "TRAIN RMSE:  0.9947249093593662\n",
      "VALID RMSE:  1.0063197505899575\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 2.2106\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.2166\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1155\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0867\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0701\n",
      "optimizer adam\n",
      "epoch 5\n",
      "embed_size 150\n",
      "TRAIN RMSE:  0.9968147669705362\n",
      "VALID RMSE:  1.0075998191642308\n",
      "Epoch 1/5\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 2.0022\n",
      "Epoch 2/5\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.1979\n",
      "Epoch 3/5\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.1047\n",
      "Epoch 4/5\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0807\n",
      "Epoch 5/5\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0705\n",
      "optimizer adam\n",
      "epoch 5\n",
      "embed_size 200\n",
      "TRAIN RMSE:  0.996825659446667\n",
      "VALID RMSE:  1.006128514459114\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 4s 1ms/step - loss: 2.1925\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2101\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.1043\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0789\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0669\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0366\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0233\n",
      "optimizer adam\n",
      "epoch 7\n",
      "embed_size 25\n",
      "TRAIN RMSE:  0.99206373658385\n",
      "VALID RMSE:  1.0044002737561781\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 2.1921\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2184\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.1111\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0824\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0686\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0389\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0283\n",
      "optimizer adam\n",
      "epoch 7\n",
      "embed_size 50\n",
      "TRAIN RMSE:  0.9910308201051224\n",
      "VALID RMSE:  1.0063231700506898\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.2787\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2164\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.1116\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0854\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0683\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0430\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0295\n",
      "optimizer adam\n",
      "epoch 7\n",
      "embed_size 100\n",
      "TRAIN RMSE:  0.9941229326367733\n",
      "VALID RMSE:  1.0074937433510607\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 2.0932\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.2103\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1134\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0869\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0707\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0413\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0328\n",
      "optimizer adam\n",
      "epoch 7\n",
      "embed_size 150\n",
      "TRAIN RMSE:  0.9932116205503541\n",
      "VALID RMSE:  1.0066121521467173\n",
      "Epoch 1/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 2.1875\n",
      "Epoch 2/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.2175\n",
      "Epoch 3/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.1130\n",
      "Epoch 4/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0881\n",
      "Epoch 5/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0736\n",
      "Epoch 6/7\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0436\n",
      "Epoch 7/7\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0332\n",
      "optimizer adam\n",
      "epoch 7\n",
      "embed_size 200\n",
      "TRAIN RMSE:  0.9945422875512987\n",
      "VALID RMSE:  1.0050228265932524\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 2.1501\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2109\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1113\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0821\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0655\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0375\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0275\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0157\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0145\n",
      "optimizer adam\n",
      "epoch 9\n",
      "embed_size 25\n",
      "TRAIN RMSE:  0.9859627308289296\n",
      "VALID RMSE:  1.0070714394930118\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.1766\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2102\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.1014\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0788\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0659\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0407\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0277\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0152\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0171\n",
      "optimizer adam\n",
      "epoch 9\n",
      "embed_size 50\n",
      "TRAIN RMSE:  0.984903077695779\n",
      "VALID RMSE:  1.0047683064223125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.1313\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2167\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1100\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0823\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0641\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0376\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0293\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0156\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0192\n",
      "optimizer adam\n",
      "epoch 9\n",
      "embed_size 100\n",
      "TRAIN RMSE:  0.9850214766891505\n",
      "VALID RMSE:  1.00707474022831\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.9705\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2024\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1061\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0830\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0676\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0415\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0333\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0194\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0210\n",
      "optimizer adam\n",
      "epoch 9\n",
      "embed_size 150\n",
      "TRAIN RMSE:  0.9870724934977515\n",
      "VALID RMSE:  1.004304131858375\n",
      "Epoch 1/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.0187\n",
      "Epoch 2/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.2142\n",
      "Epoch 3/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.1160\n",
      "Epoch 4/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0880\n",
      "Epoch 5/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0735\n",
      "Epoch 6/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0435\n",
      "Epoch 7/9\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0369\n",
      "Epoch 8/9\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0210\n",
      "Epoch 9/9\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0228\n",
      "optimizer adam\n",
      "epoch 9\n",
      "embed_size 200\n",
      "TRAIN RMSE:  0.9867951048727103\n",
      "VALID RMSE:  1.0068925988405988\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 2.2030\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.2160\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.1157\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0858\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0707\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0357\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0239\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0132\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0136\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0073\n",
      "optimizer adam\n",
      "epoch 10\n",
      "embed_size 25\n",
      "TRAIN RMSE:  0.9890105378831081\n",
      "VALID RMSE:  1.0077424161915112\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.2180\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2189\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1096\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0827\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0654\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0397\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0256\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0160\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0157\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0101\n",
      "optimizer adam\n",
      "epoch 10\n",
      "embed_size 50\n",
      "TRAIN RMSE:  0.9856199021037522\n",
      "VALID RMSE:  1.0078314867735643\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.0955\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2019\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1019\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0816\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0697\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0439\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0295\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0180\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0191\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0147\n",
      "optimizer adam\n",
      "epoch 10\n",
      "embed_size 100\n",
      "TRAIN RMSE:  0.9871917172562639\n",
      "VALID RMSE:  1.0059786037055116\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.0821\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2175\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1124\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0859\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0672\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0424\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0303\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0222\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0195\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0151\n",
      "optimizer adam\n",
      "epoch 10\n",
      "embed_size 150\n",
      "TRAIN RMSE:  0.9866015890097155\n",
      "VALID RMSE:  1.00797058592894\n",
      "Epoch 1/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.1097\n",
      "Epoch 2/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.2130\n",
      "Epoch 3/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.1088\n",
      "Epoch 4/10\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0854\n",
      "Epoch 5/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0699\n",
      "Epoch 6/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0459\n",
      "Epoch 7/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0352\n",
      "Epoch 8/10\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0206\n",
      "Epoch 9/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0268\n",
      "Epoch 10/10\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0224\n",
      "optimizer adam\n",
      "epoch 10\n",
      "embed_size 200\n",
      "TRAIN RMSE:  0.9898548764547723\n",
      "VALID RMSE:  1.0118813828523363\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 2.2733\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.2097\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.1090\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0822\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0681\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0368\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0273\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0117\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0150\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0086\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0068\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0013\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9987\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 1.0041\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 3s 1ms/step - loss: 0.9874\n",
      "optimizer adam\n",
      "epoch 15\n",
      "embed_size 25\n",
      "TRAIN RMSE:  0.9779276838599867\n",
      "VALID RMSE:  1.0103750483881286\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.2238\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2172\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1172\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0848\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0654\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0394\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0273\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0118\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0112\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0088\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0040\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9964\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9975\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0001\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9860\n",
      "optimizer adam\n",
      "epoch 15\n",
      "embed_size 50\n",
      "TRAIN RMSE:  0.9784721310124329\n",
      "VALID RMSE:  1.008812542937975\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 2.1293\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.2099\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.1053\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0853\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0644\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0387\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0271\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0149\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0175\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0125\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0063\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 0.9999\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9988\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 1.0027\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 3s 2ms/step - loss: 0.9897\n",
      "optimizer adam\n",
      "epoch 15\n",
      "embed_size 100\n",
      "TRAIN RMSE:  0.9765960069165762\n",
      "VALID RMSE:  1.0097073923580282\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 2.0686\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.2053\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.1091\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0860\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0717\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0433\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0333\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0182\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0206\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.0191\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0136\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0034\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0032\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0076\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 0.9916\n",
      "optimizer adam\n",
      "epoch 15\n",
      "embed_size 150\n",
      "TRAIN RMSE:  0.9787323183074166\n",
      "VALID RMSE:  1.0098169724487915\n",
      "Epoch 1/15\n",
      "1878/1878 [==============================] - 7s 3ms/step - loss: 2.0921\n",
      "Epoch 2/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.2134\n",
      "Epoch 3/15\n",
      "1878/1878 [==============================] - 5s 3ms/step - loss: 1.1116\n",
      "Epoch 4/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0823\n",
      "Epoch 5/15\n",
      "1878/1878 [==============================] - 4s 2ms/step - loss: 1.0670\n",
      "Epoch 6/15\n",
      "1878/1878 [==============================] - 8s 4ms/step - loss: 1.0413\n",
      "Epoch 7/15\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0324\n",
      "Epoch 8/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0223\n",
      "Epoch 9/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0240\n",
      "Epoch 10/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0181\n",
      "Epoch 11/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0124\n",
      "Epoch 12/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0108\n",
      "Epoch 13/15\n",
      "1878/1878 [==============================] - 6s 3ms/step - loss: 1.0061\n",
      "Epoch 14/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 1.0108\n",
      "Epoch 15/15\n",
      "1878/1878 [==============================] - 5s 2ms/step - loss: 0.9943\n",
      "optimizer adam\n",
      "epoch 15\n",
      "embed_size 200\n",
      "TRAIN RMSE:  0.9802082803151463\n",
      "VALID RMSE:  1.0072385005021534\n"
     ]
    }
   ],
   "source": [
    "Train_RMSE = []\n",
    "Valid_RMSE = []\n",
    "train_optimizer = []\n",
    "train_epoch = []\n",
    "train_embed_size = []\n",
    "\n",
    "optimizer = ['adagrad', 'adam']\n",
    "epoch =  [3,5,7,9,10,15]\n",
    "embed_size = [25,50,100,150,200]\n",
    "\n",
    "for o in optimizer:\n",
    "    for e in epoch:\n",
    "        for es in embed_size:\n",
    "            wdl_model = build_wdl_model(\n",
    "            len(tr_continuous_features[0]),\n",
    "            item_deep_vocab_lens,   # num of category classes\n",
    "            len(tr_wide_features[0]), \n",
    "            embed_size=es)\n",
    "            \n",
    "            wdl_model.compile(optimizer=o, loss='mse')\n",
    "\n",
    "            history = wdl_model.fit(\n",
    "                tr_features, \n",
    "                tr_ratings, \n",
    "                epochs=e, verbose=1, callbacks=[ModelCheckpoint('model.h5')])\n",
    "            \n",
    "            print(\"optimizer\",o)\n",
    "            print(\"epoch\",e)\n",
    "            print(\"embed_size\",es)\n",
    "            y_pred = wdl_model.predict(tr_features)\n",
    "            print(\"TRAIN RMSE: \", rmse(y_pred, tr_ratings))\n",
    "            Train_RMSE.append(rmse(y_pred, tr_ratings))\n",
    "            y_pred = wdl_model.predict(val_features)\n",
    "            print(\"VALID RMSE: \", rmse(y_pred, val_ratings))\n",
    "            train_optimizer.append(o)\n",
    "            train_epoch.append(e)\n",
    "            train_embed_size.append(es)\n",
    "            Valid_RMSE.append(rmse(y_pred, val_ratings))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "   \n",
    "result['optimizer'] = train_optimizer\n",
    "result['epoch '] = train_epoch \n",
    "result['embed_size'] = train_embed_size\n",
    "result['Train_RMSE'] = Train_RMSE\n",
    "result['Valid_RMSE'] = Valid_RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.sort_values(by=['Valid_RMSE'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>optimizer</th>\n",
       "      <th>epoch</th>\n",
       "      <th>embed_size</th>\n",
       "      <th>Train_RMSE</th>\n",
       "      <th>Valid_RMSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>adam</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>0.987072</td>\n",
       "      <td>1.004304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>adam</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>0.992064</td>\n",
       "      <td>1.004400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>adam</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>0.984903</td>\n",
       "      <td>1.004768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>adam</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>0.994542</td>\n",
       "      <td>1.005023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0.994214</td>\n",
       "      <td>1.005433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>0.987192</td>\n",
       "      <td>1.005979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>0.996826</td>\n",
       "      <td>1.006129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>0.994725</td>\n",
       "      <td>1.006320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>adam</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>0.991031</td>\n",
       "      <td>1.006323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>adam</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>0.993212</td>\n",
       "      <td>1.006612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>adam</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "      <td>0.986795</td>\n",
       "      <td>1.006893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>adam</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>0.985963</td>\n",
       "      <td>1.007071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>adam</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>0.985021</td>\n",
       "      <td>1.007075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>adam</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>0.980208</td>\n",
       "      <td>1.007239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>1.016744</td>\n",
       "      <td>1.007332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>adam</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0.994123</td>\n",
       "      <td>1.007494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>0.994159</td>\n",
       "      <td>1.007547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>adam</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>0.996815</td>\n",
       "      <td>1.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1.002686</td>\n",
       "      <td>1.007648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>0.989011</td>\n",
       "      <td>1.007742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>0.985620</td>\n",
       "      <td>1.007831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>0.986602</td>\n",
       "      <td>1.007971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>1.003706</td>\n",
       "      <td>1.008430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>adam</td>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>0.978472</td>\n",
       "      <td>1.008813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>1.004168</td>\n",
       "      <td>1.008896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.002095</td>\n",
       "      <td>1.009117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>1.016810</td>\n",
       "      <td>1.009580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>adam</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>0.976596</td>\n",
       "      <td>1.009707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>adam</td>\n",
       "      <td>15</td>\n",
       "      <td>150</td>\n",
       "      <td>0.978732</td>\n",
       "      <td>1.009817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>adam</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>0.977928</td>\n",
       "      <td>1.010375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>adam</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>0.989855</td>\n",
       "      <td>1.011881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>1.021273</td>\n",
       "      <td>1.012311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>9</td>\n",
       "      <td>200</td>\n",
       "      <td>1.020469</td>\n",
       "      <td>1.012432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>7</td>\n",
       "      <td>200</td>\n",
       "      <td>1.025844</td>\n",
       "      <td>1.013084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>1.022803</td>\n",
       "      <td>1.013370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>adam</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1.006646</td>\n",
       "      <td>1.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1.023916</td>\n",
       "      <td>1.014135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>9</td>\n",
       "      <td>100</td>\n",
       "      <td>1.026496</td>\n",
       "      <td>1.015179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>1.027307</td>\n",
       "      <td>1.016039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>150</td>\n",
       "      <td>1.024411</td>\n",
       "      <td>1.016739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>15</td>\n",
       "      <td>50</td>\n",
       "      <td>1.024112</td>\n",
       "      <td>1.017069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>9</td>\n",
       "      <td>150</td>\n",
       "      <td>1.026006</td>\n",
       "      <td>1.017524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>1.028831</td>\n",
       "      <td>1.017658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>1.030493</td>\n",
       "      <td>1.018895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>1.033554</td>\n",
       "      <td>1.020346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>150</td>\n",
       "      <td>1.031037</td>\n",
       "      <td>1.022234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>7</td>\n",
       "      <td>150</td>\n",
       "      <td>1.032616</td>\n",
       "      <td>1.023672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>1.032584</td>\n",
       "      <td>1.023757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>1.035368</td>\n",
       "      <td>1.023862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>1.039907</td>\n",
       "      <td>1.025731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>1.039423</td>\n",
       "      <td>1.026867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>1.039674</td>\n",
       "      <td>1.027138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>1.042614</td>\n",
       "      <td>1.029016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>200</td>\n",
       "      <td>1.051699</td>\n",
       "      <td>1.032908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>1.046322</td>\n",
       "      <td>1.034630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>1.050099</td>\n",
       "      <td>1.038115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>1.050564</td>\n",
       "      <td>1.038589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>1.062356</td>\n",
       "      <td>1.047738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>1.067114</td>\n",
       "      <td>1.051709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adagrad</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>1.082213</td>\n",
       "      <td>1.067034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   optimizer  epoch   embed_size  Train_RMSE  Valid_RMSE\n",
       "48      adam       9         150    0.987072    1.004304\n",
       "40      adam       7          25    0.992064    1.004400\n",
       "46      adam       9          50    0.984903    1.004768\n",
       "44      adam       7         200    0.994542    1.005023\n",
       "35      adam       5          25    0.994214    1.005433\n",
       "52      adam      10         100    0.987192    1.005979\n",
       "39      adam       5         200    0.996826    1.006129\n",
       "37      adam       5         100    0.994725    1.006320\n",
       "41      adam       7          50    0.991031    1.006323\n",
       "43      adam       7         150    0.993212    1.006612\n",
       "49      adam       9         200    0.986795    1.006893\n",
       "45      adam       9          25    0.985963    1.007071\n",
       "47      adam       9         100    0.985021    1.007075\n",
       "59      adam      15         200    0.980208    1.007239\n",
       "28   adagrad      15         150    1.016744    1.007332\n",
       "42      adam       7         100    0.994123    1.007494\n",
       "36      adam       5          50    0.994159    1.007547\n",
       "38      adam       5         150    0.996815    1.007600\n",
       "30      adam       3          25    1.002686    1.007648\n",
       "50      adam      10          25    0.989011    1.007742\n",
       "51      adam      10          50    0.985620    1.007831\n",
       "53      adam      10         150    0.986602    1.007971\n",
       "33      adam       3         150    1.003706    1.008430\n",
       "56      adam      15          50    0.978472    1.008813\n",
       "31      adam       3          50    1.004168    1.008896\n",
       "32      adam       3         100    1.002095    1.009117\n",
       "29   adagrad      15         200    1.016810    1.009580\n",
       "57      adam      15         100    0.976596    1.009707\n",
       "58      adam      15         150    0.978732    1.009817\n",
       "55      adam      15          25    0.977928    1.010375\n",
       "54      adam      10         200    0.989855    1.011881\n",
       "27   adagrad      15         100    1.021273    1.012311\n",
       "19   adagrad       9         200    1.020469    1.012432\n",
       "14   adagrad       7         200    1.025844    1.013084\n",
       "24   adagrad      10         200    1.022803    1.013370\n",
       "34      adam       3         200    1.006646    1.013600\n",
       "22   adagrad      10         100    1.023916    1.014135\n",
       "17   adagrad       9         100    1.026496    1.015179\n",
       "25   adagrad      15          25    1.027307    1.016039\n",
       "23   adagrad      10         150    1.024411    1.016739\n",
       "26   adagrad      15          50    1.024112    1.017069\n",
       "18   adagrad       9         150    1.026006    1.017524\n",
       "9    adagrad       5         200    1.028831    1.017658\n",
       "21   adagrad      10          50    1.030493    1.018895\n",
       "12   adagrad       7         100    1.033554    1.020346\n",
       "8    adagrad       5         150    1.031037    1.022234\n",
       "13   adagrad       7         150    1.032616    1.023672\n",
       "16   adagrad       9          50    1.032584    1.023757\n",
       "20   adagrad      10          25    1.035368    1.023862\n",
       "11   adagrad       7          50    1.039907    1.025731\n",
       "15   adagrad       9          25    1.039423    1.026867\n",
       "7    adagrad       5         100    1.039674    1.027138\n",
       "10   adagrad       7          25    1.042614    1.029016\n",
       "4    adagrad       3         200    1.051699    1.032908\n",
       "6    adagrad       5          50    1.046322    1.034630\n",
       "5    adagrad       5          25    1.050099    1.038115\n",
       "3    adagrad       3         150    1.050564    1.038589\n",
       "2    adagrad       3         100    1.062356    1.047738\n",
       "1    adagrad       3          50    1.067114    1.051709\n",
       "0    adagrad       3          25    1.082213    1.067034"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
